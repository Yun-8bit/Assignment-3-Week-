{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcwbd63QsuzdtrFea6vFkm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yun-8bit/Assignment-3-Week-/blob/main/%EA%B3%BC%EC%A0%9C_11%EC%A3%BC%EC%B0%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "과제 1번 Training"
      ],
      "metadata": {
        "id": "1Vqa-rFgxbdq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjp3d6XbxVdW"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(train_x, train_y), (test_x, test_y) = mnist.load_data()\n",
        "\n",
        "train_x, test_x = train_x/255.0, test_x/255.0"
      ],
      "metadata": {
        "id": "rYhMEoS-xe8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x.shape"
      ],
      "metadata": {
        "id": "DrmuuzNNxgPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = train_x.reshape((train_x.shape[0], 28, 28, 1))\n",
        "test_x = test_x.reshape((test_x.shape[0], 28, 28, 1))"
      ],
      "metadata": {
        "id": "A6FCzD01xhXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input layer\n",
        "input_h =  28  # input height\n",
        "input_w =  28  # input width\n",
        "input_ch =  1  # input channel : Gray scale\n",
        "\n",
        "\n",
        "# 1st convolution layer\n",
        "k1_h = 3\n",
        "k1_w = 3\n",
        "k1_ch = 32\n",
        "p1_h = 2\n",
        "p1_w = 2\n",
        "# (None, 14, 14, 32)\n",
        "\n",
        "# 2nd convolution layer\n",
        "k2_h = 3\n",
        "k2_w = 3\n",
        "k2_ch = 64\n",
        "p2_h = 2\n",
        "p2_w = 2\n",
        "# (None, 7, 7, 64)\n",
        "\n",
        "# Fully connected : flatten the features (7x7x64)\n",
        "conv_result_size = 7*7*64\n",
        "n_hidden = 100\n",
        "n_output = 10"
      ],
      "metadata": {
        "id": "7lbYqrH9xhzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kernel size : [kernel_height, kernel_width, input_ch, output_ch]\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "weights = {\n",
        "    'conv1' : tf.Variable(tf.random.normal([k1_h, k1_w, input_ch, k1_ch], stddev = 0.1)),\n",
        "    'conv2' : tf.Variable(tf.random.normal([k2_h, k2_w, k1_ch, k2_ch], stddev = 0.1)),\n",
        "    'hidden' : tf.Variable(tf.random.normal([conv_result_size, n_hidden], stddev = 0.1)),\n",
        "    'output' : tf.Variable(tf.random.normal([n_hidden, n_output], stddev = 0.1))\n",
        "}\n",
        "\n",
        "# bias size : [output_ch] or [neuron_size]\n",
        "biases = {\n",
        "    'conv1' : tf.Variable(tf.random.normal([k1_ch], stddev = 0.1)),\n",
        "    'conv2' : tf.Variable(tf.random.normal([k2_ch], stddev = 0.1)),\n",
        "    'hidden' : tf.Variable(tf.random.normal([n_hidden], stddev = 0.1)),\n",
        "    'output' : tf.Variable(tf.random.normal([n_output], stddev = 0.1))\n",
        "}\n",
        "\n",
        "# input layer : [batch_size, image_height, image_width, channels]\n",
        "# output layer : [batch_size, class_size]\n",
        "x = tf.placeholder(shape=[None, input_h, input_w, input_ch], dtype=tf.float32)\n",
        "y = tf.placeholder(shape=[None, n_output], dtype=tf.float32)\n"
      ],
      "metadata": {
        "id": "35i7sU6-xiCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [batch, height, width, channels]\n",
        "\n",
        "def net(x, weights, biases):\n",
        "    # 1st convolution layer\n",
        "    conv1 = tf.nn.conv2d(x, weights['conv1'], strides=[1, 1, 1, 1], padding='SAME')\n",
        "    conv1 = tf.nn.relu(tf.add(conv1, biases['conv1']))\n",
        "    maxp1 = tf.nn.max_pool(conv1, ksize=[1, p1_h, p1_w, 1], strides=[1, p1_h, p1_w, 1], padding='SAME')\n",
        "#   maxp1 = tf.nn.max_pool(conv1, ksize=[1,p1_h,p1_w,1], strides=[1,p1_h,p1_w,1], padding='VALID')\n",
        "\n",
        "    # 2nd convolution layer\n",
        "    conv2 = tf.nn.conv2d(maxp1, weights['conv2'], strides=[1, 1, 1, 1], padding='SAME')\n",
        "    conv2 = tf.nn.relu(tf.add(conv2, biases['conv2']))\n",
        "    maxp2 = tf.nn.max_pool(conv2, ksize=[1, p2_h, p2_w, 1], strides=[1, p2_h, p2_w, 1], padding='SAME')\n",
        "\n",
        "    maxp2_flatten = tf.reshape(maxp2, [-1, conv_result_size])\n",
        "\n",
        "    # Fully connected layer\n",
        "    hidden_fc = tf.matmul(maxp2_flatten, weights['hidden'])\n",
        "    hidden = tf.nn.relu(tf.add(hidden_fc, biases['hidden']))\n",
        "\n",
        "    output_fc = tf.matmul(hidden, weights['output'])\n",
        "    output = tf.add(output_fc, biases['output'])\n",
        "\n",
        "    return output\n",
        ""
      ],
      "metadata": {
        "id": "FF3nJWZFxiL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function\n",
        "\n",
        "LR = 0.0001\n",
        "pred = net(x, weights, biases)\n",
        "loss = tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y)\n",
        "loss = tf.reduce_mean(loss)\n",
        "\n",
        "optm = tf.train.AdamOptimizer(learning_rate=LR).minimize(loss)"
      ],
      "metadata": {
        "id": "eA4m7sKUxmus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimization\n",
        "\n",
        "n_batch = 50\n",
        "n_iter = 2500\n",
        "n_prt = 250\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "loss_record_train = []\n",
        "loss_record_test = []\n",
        "for epoch in range(n_iter):\n",
        "    # train_x, train_y = mnist.train.next_batch(n_batch)\n",
        "    # train_x = np.reshape(train_x, (-1, 28, 28, 1))\n",
        "    # # train_x = np.reshape(train_x, (n_batch, input_h, input_w, input_ch))\n",
        "    # sess.run(optm, feed_dict={x:train_x, y:train_y})\n",
        "\n",
        "    # Get a random batch of training data\n",
        "    batch_indices = np.random.choice(train_x.shape[0], size=n_batch, replace=False)\n",
        "    batch_x = train_x[batch_indices]\n",
        "    batch_y = train_y[batch_indices]\n",
        "\n",
        "    batch_x = np.reshape(batch_x, (-1, 28, 28, 1))\n",
        "    # batch_x = np.reshape(batch_x, (n_batch, input_h, input_w, input_ch))\n",
        "    # Convert labels to one-hot encoding if necessary\n",
        "    batch_y = tf.keras.utils.to_categorical(batch_y, num_classes=10)  # Adjust num_classes if needed\n",
        "\n",
        "    sess.run(optm, feed_dict={x:batch_x, y:batch_y})\n",
        "\n",
        "    if epoch % n_prt == 0:\n",
        "        # test_x, test_y = mnist.test.next_batch(n_batch)\n",
        "        # test_x = test_x.reshape((n_batch, input_h, input_w, input_ch))\n",
        "        # c1 = sess.run(loss, feed_dict={x:train_x, y:train_y})\n",
        "        # c2 = sess.run(loss, feed_dict={x:test_x, y:test_y})\n",
        "\n",
        "        # Get a random batch of test data\n",
        "        test_batch_indices = np.random.choice(test_x.shape[0], size=n_batch, replace=False)\n",
        "        test_batch_x = test_x[test_batch_indices]\n",
        "        test_batch_y = test_y[test_batch_indices]\n",
        "\n",
        "        test_batch_x = test_batch_x.reshape((n_batch, input_h, input_w, input_ch))\n",
        "        # Convert labels to one-hot encoding if necessary\n",
        "        test_batch_y = tf.keras.utils.to_categorical(test_batch_y, num_classes=10)  # Adjust num_classes if needed\n",
        "        c1 = sess.run(loss, feed_dict={x: batch_x, y: batch_y})\n",
        "        c2 = sess.run(loss, feed_dict={x: test_batch_x, y: test_batch_y})\n",
        "\n",
        "        loss_record_train.append(c1)\n",
        "        loss_record_test.append(c2)\n",
        "        print(\"Iter : {}\".format(epoch))\n",
        "        print(\"Train Loss : {}\".format(c1))\n",
        "        print(\"Cost : {}\".format(c2))\n",
        "\n",
        "plt.figure(figsize = (10,8))\n",
        "plt.plot(np.arange(len(loss_record_train))*n_prt, loss_record_train, 'r', label = 'Train')\n",
        "plt.plot(np.arange(len(loss_record_test))*n_prt, loss_record_test, 'b', label = 'Test')\n",
        "plt.xlabel('Iteration', fontsize = 15)\n",
        "plt.ylabel('Loss', fontsize = 15)\n",
        "plt.legend(fontsize = 12)\n",
        "plt.ylim([0, np.max(loss_record_train)])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3AIF6dJDxoA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "과제 1번 Testing or Evaluating"
      ],
      "metadata": {
        "id": "PLK7jw4cxsr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "# Get a random batch of test data\n",
        "test_batch_indices = np.random.choice(test_x.shape[0], size=n_batch, replace=False)\n",
        "test_batch_x = test_x[test_batch_indices]\n",
        "test_batch_y = test_y[test_batch_indices]\n",
        "\n",
        "test_batch_x = test_batch_x.reshape((n_batch, input_h, input_w, input_ch))  # Reshape the batch if needed\n",
        "# Convert labels to one-hot encoding if necessary\n",
        "test_batch_y = tf.keras.utils.to_categorical(test_batch_y, num_classes=10)  # Adjust num_classes if needed\n",
        "\n",
        "my_pred = sess.run(pred, feed_dict={x: test_batch_x.reshape(-1, 28, 28, 1)})\n",
        "my_pred = np.argmax(my_pred, axis=1)\n",
        "\n",
        "labels = np.argmax(test_batch_y, axis=1)\n",
        "\n",
        "accr = np.mean(np.equal(my_pred, labels))\n",
        "print(\"Accuracy : {}\".format(accr*100))"
      ],
      "metadata": {
        "id": "S95--IjfxoKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a random batch of test data\n",
        "n_batch = 1\n",
        "test_batch_indices = np.random.choice(test_x.shape[0], size=n_batch, replace=False)\n",
        "test_batch_x = test_x[test_batch_indices]\n",
        "test_batch_y = test_y[test_batch_indices]\n",
        "\n",
        "test_batch_x = test_batch_x.reshape((n_batch, input_h, input_w, input_ch))  # Reshape the batch if needed\n",
        "# Convert labels to one-hot encoding if necessary\n",
        "test_batch_y = tf.keras.utils.to_categorical(test_batch_y, num_classes=10)  # Adjust num_classes if needed\n",
        "\n",
        "logits = sess.run(tf.nn.softmax(pred), feed_dict={x: test_batch_x.reshape(-1, 28, 28, 1)})\n",
        "predict = np.argmax(logits)\n",
        "\n",
        "plt.figure(figsize = (12, 5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(test_batch_x.reshape(28, 28), 'gray')\n",
        "plt.axis('off')\n",
        "plt.subplot(1,2,2)\n",
        "plt.stem(logits.ravel())\n",
        "plt.show()\n",
        "\n",
        "np.set_printoptions(precision = 2, suppress = True)\n",
        "print('Prediction : {}'.format(predict))\n",
        "print('Probability : {}'.format(logits.ravel()))"
      ],
      "metadata": {
        "id": "F4QJyVSMxoXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "과제 2번 Training"
      ],
      "metadata": {
        "id": "yQMKjDd4yBth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "BDbT6LGOxp_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(train_x, train_y), (test_x, test_y) = mnist.load_data()\n",
        "\n",
        "train_x, test_x = train_x/255.0, test_x/255.0"
      ],
      "metadata": {
        "id": "IESgb00nxqHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x.shape"
      ],
      "metadata": {
        "id": "w-UHLQRDxqTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = train_x.reshape((train_x.shape[0], 28, 28, 1))\n",
        "test_x = test_x.reshape((test_x.shape[0], 28, 28, 1))"
      ],
      "metadata": {
        "id": "-cYrXgPRxqjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input layer\n",
        "input_h =  28  # input height\n",
        "input_w =  28  # input width\n",
        "input_ch =  1  # input channel : Gray scale\n",
        "\n",
        "\n",
        "# 1st convolution layer\n",
        "k1_h = 3\n",
        "k1_w = 3\n",
        "k1_ch = 32\n",
        "p1_h = 2\n",
        "p1_w = 2\n",
        "# (None, 14, 14, 32)\n",
        "\n",
        "# 2nd convolution layer\n",
        "k2_h = 3\n",
        "k2_w = 3\n",
        "k2_ch = 64\n",
        "p2_h = 2\n",
        "p2_w = 2\n",
        "# (None, 7, 7, 64)\n",
        "\n",
        "# 3nd convolution layer\n",
        "k3_h = 3\n",
        "k3_w = 3\n",
        "k3_ch = 128\n",
        "p3_h = 2\n",
        "p3_w = 2\n",
        "# (None, 4, 4, 128)\n",
        "\n",
        "# Fully connected : flatten the features (4x4x128)\n",
        "conv_result_size = 4*4*128\n",
        "n_hidden = 100\n",
        "n_output = 10"
      ],
      "metadata": {
        "id": "RqhMBs25xxDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kernel size : [kernel_height, kernel_width, input_ch, output_ch]\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "weights = {\n",
        "    'conv1' : tf.Variable(tf.random.normal([k1_h, k1_w, input_ch, k1_ch], stddev = 0.1)),\n",
        "    'conv2' : tf.Variable(tf.random.normal([k2_h, k2_w, k1_ch, k2_ch], stddev = 0.1)),\n",
        "    'conv3': tf.Variable(tf.random.normal([k3_h, k3_w, k2_ch, k3_ch], stddev=0.1)),\n",
        "    'fc2' : tf.Variable(tf.random.normal([conv_result_size, n_hidden], stddev = 0.1)),\n",
        "    'output2' : tf.Variable(tf.random.normal([n_hidden, n_output], stddev = 0.1))\n",
        "}\n",
        "\n",
        "# bias size : [output_ch] or [neuron_size]\n",
        "biases = {\n",
        "    'conv1' : tf.Variable(tf.random.normal([k1_ch], stddev = 0.1)),\n",
        "    'conv2' : tf.Variable(tf.random.normal([k2_ch], stddev = 0.1)),\n",
        "    'conv3': tf.Variable(tf.random.normal([k3_ch], stddev=0.1)),\n",
        "    'fc2' : tf.Variable(tf.random.normal([n_hidden], stddev = 0.1)),\n",
        "    'output2' : tf.Variable(tf.random.normal([n_output], stddev = 0.1))\n",
        "}\n",
        "\n",
        "# input layer : [batch_size, image_height, image_width, channels]\n",
        "# output layer : [batch_size, class_size]\n",
        "x = tf.placeholder(shape=[None, input_h, input_w, input_ch], dtype=tf.float32)\n",
        "y = tf.placeholder(shape=[None, n_output], dtype=tf.float32)\n"
      ],
      "metadata": {
        "id": "-FMFRAGsxxLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [batch, height, width, channels]\n",
        "\n",
        "def net(x, weights, biases):\n",
        "    # 1st convolution layer\n",
        "    conv1 = tf.nn.conv2d(x, weights['conv1'], strides=[1, 1, 1, 1], padding='SAME')\n",
        "    conv1 = tf.nn.relu(tf.add(conv1, biases['conv1']))\n",
        "    maxp1 = tf.nn.max_pool(conv1, ksize=[1, p1_h, p1_w, 1], strides=[1, p1_h, p1_w, 1], padding='SAME')\n",
        "#   maxp1 = tf.nn.max_pool(conv1, ksize=[1,p1_h,p1_w,1], strides=[1,p1_h,p1_w,1], padding='VALID')\n",
        "\n",
        "    # 2nd convolution layer\n",
        "    conv2 = tf.nn.conv2d(maxp1, weights['conv2'], strides=[1, 1, 1, 1], padding='SAME')\n",
        "    conv2 = tf.nn.relu(tf.add(conv2, biases['conv2']))\n",
        "    maxp2 = tf.nn.max_pool(conv2, ksize=[1, p2_h, p2_w, 1], strides=[1, p2_h, p2_w, 1], padding='SAME')\n",
        "\n",
        "    # 3nd convolution layer\n",
        "    conv3 = tf.nn.conv2d(maxp2, weights['conv3'], strides=[1,1,1,1], padding='SAME')\n",
        "    conv3 = tf.nn.relu(tf.add(conv3, biases['conv3']))\n",
        "    maxp3 = tf.nn.max_pool(conv3, ksize=[1,p3_h,p3_w,1], strides=[1,p3_h,p3_w,1], padding='SAME')\n",
        "\n",
        "    maxp3_flatten = tf.reshape(maxp3, [-1, conv_result_size])\n",
        "\n",
        "    # Fully connected layer\n",
        "    fc1_fc = tf.matmul(maxp3_flatten, weights['fc2'])\n",
        "    fc1 = tf.nn.relu(tf.add(fc1_fc, biases['fc2']))\n",
        "\n",
        "    output_fc = tf.matmul(fc1, weights['output2'])\n",
        "    output = tf.add(output_fc, biases['output2'])\n",
        "\n",
        "    return output\n",
        ""
      ],
      "metadata": {
        "id": "QeI2mtHUxxY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function\n",
        "\n",
        "LR = 0.0001\n",
        "pred = net(x, weights, biases)\n",
        "loss = tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y)\n",
        "loss = tf.reduce_mean(loss)\n",
        "\n",
        "optm = tf.train.AdamOptimizer(learning_rate=LR).minimize(loss)"
      ],
      "metadata": {
        "id": "pc6Eet59yMrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimization\n",
        "\n",
        "n_batch = 50\n",
        "n_iter = 2500\n",
        "n_prt = 250\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "loss_record_train = []\n",
        "loss_record_test = []\n",
        "for epoch in range(n_iter):\n",
        "    # train_x, train_y = mnist.train.next_batch(n_batch)\n",
        "    # train_x = np.reshape(train_x, (-1, 28, 28, 1))\n",
        "    # # train_x = np.reshape(train_x, (n_batch, input_h, input_w, input_ch))\n",
        "    # sess.run(optm, feed_dict={x:train_x, y:train_y})\n",
        "\n",
        "    # Get a random batch of training data\n",
        "    batch_indices = np.random.choice(train_x.shape[0], size=n_batch, replace=False)\n",
        "    batch_x = train_x[batch_indices]\n",
        "    batch_y = train_y[batch_indices]\n",
        "\n",
        "    batch_x = np.reshape(batch_x, (-1, 28, 28, 1))\n",
        "    # batch_x = np.reshape(batch_x, (n_batch, input_h, input_w, input_ch))\n",
        "    # Convert labels to one-hot encoding if necessary\n",
        "    batch_y = tf.keras.utils.to_categorical(batch_y, num_classes=10)  # Adjust num_classes if needed\n",
        "\n",
        "    sess.run(optm, feed_dict={x:batch_x, y:batch_y})\n",
        "\n",
        "    if epoch % n_prt == 0:\n",
        "        # test_x, test_y = mnist.test.next_batch(n_batch)\n",
        "        # test_x = test_x.reshape((n_batch, input_h, input_w, input_ch))\n",
        "        # c1 = sess.run(loss, feed_dict={x:train_x, y:train_y})\n",
        "        # c2 = sess.run(loss, feed_dict={x:test_x, y:test_y})\n",
        "\n",
        "        # Get a random batch of test data\n",
        "        test_batch_indices = np.random.choice(test_x.shape[0], size=n_batch, replace=False)\n",
        "        test_batch_x = test_x[test_batch_indices]\n",
        "        test_batch_y = test_y[test_batch_indices]\n",
        "\n",
        "        test_batch_x = test_batch_x.reshape((n_batch, input_h, input_w, input_ch))\n",
        "        # Convert labels to one-hot encoding if necessary\n",
        "        test_batch_y = tf.keras.utils.to_categorical(test_batch_y, num_classes=10)  # Adjust num_classes if needed\n",
        "        c1 = sess.run(loss, feed_dict={x: batch_x, y: batch_y})\n",
        "        c2 = sess.run(loss, feed_dict={x: test_batch_x, y: test_batch_y})\n",
        "\n",
        "        loss_record_train.append(c1)\n",
        "        loss_record_test.append(c2)\n",
        "        print(\"Iter : {}\".format(epoch))\n",
        "        print(\"Train Loss : {}\".format(c1))\n",
        "        print(\"Cost : {}\".format(c2))\n",
        "\n",
        "plt.figure(figsize = (10,8))\n",
        "plt.plot(np.arange(len(loss_record_train))*n_prt, loss_record_train, 'r', label = 'Train')\n",
        "plt.plot(np.arange(len(loss_record_test))*n_prt, loss_record_test, 'b', label = 'Test')\n",
        "plt.xlabel('Iteration', fontsize = 15)\n",
        "plt.ylabel('Loss', fontsize = 15)\n",
        "plt.legend(fontsize = 12)\n",
        "plt.ylim([0, np.max(loss_record_train)])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M-PpaCTdyMzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "과제 2번 Testing or Evaluating"
      ],
      "metadata": {
        "id": "BpwgFsOuyWXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "# Get a random batch of test data\n",
        "test_batch_indices = np.random.choice(test_x.shape[0], size=n_batch, replace=False)\n",
        "test_batch_x = test_x[test_batch_indices]\n",
        "test_batch_y = test_y[test_batch_indices]\n",
        "\n",
        "test_batch_x = test_batch_x.reshape((n_batch, input_h, input_w, input_ch))  # Reshape the batch if needed\n",
        "# Convert labels to one-hot encoding if necessary\n",
        "test_batch_y = tf.keras.utils.to_categorical(test_batch_y, num_classes=10)  # Adjust num_classes if needed\n",
        "\n",
        "my_pred = sess.run(pred, feed_dict={x: test_batch_x.reshape(-1, 28, 28, 1)})\n",
        "my_pred = np.argmax(my_pred, axis=1)\n",
        "\n",
        "labels = np.argmax(test_batch_y, axis=1)\n",
        "\n",
        "accr = np.mean(np.equal(my_pred, labels))\n",
        "print(\"Accuracy : {}\".format(accr*100))"
      ],
      "metadata": {
        "id": "5kSVOLGZyM7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a random batch of test data\n",
        "n_batch = 1\n",
        "test_batch_indices = np.random.choice(test_x.shape[0], size=n_batch, replace=False)\n",
        "test_batch_x = test_x[test_batch_indices]\n",
        "test_batch_y = test_y[test_batch_indices]\n",
        "\n",
        "test_batch_x = test_batch_x.reshape((n_batch, input_h, input_w, input_ch))  # Reshape the batch if needed\n",
        "# Convert labels to one-hot encoding if necessary\n",
        "test_batch_y = tf.keras.utils.to_categorical(test_batch_y, num_classes=10)  # Adjust num_classes if needed\n",
        "\n",
        "logits = sess.run(tf.nn.softmax(pred), feed_dict={x: test_batch_x.reshape(-1, 28, 28, 1)})\n",
        "predict = np.argmax(logits)\n",
        "\n",
        "plt.figure(figsize = (12, 5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(test_batch_x.reshape(28, 28), 'gray')\n",
        "plt.axis('off')\n",
        "plt.subplot(1,2,2)\n",
        "plt.stem(logits.ravel())\n",
        "plt.show()\n",
        "\n",
        "np.set_printoptions(precision = 2, suppress = True)\n",
        "print('Prediction : {}'.format(predict))\n",
        "print('Probability : {}'.format(logits.ravel()))"
      ],
      "metadata": {
        "id": "TceuKZfAyNF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8vH-6oIuyNPe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}